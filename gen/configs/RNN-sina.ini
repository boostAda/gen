[model]
model_type=RNN
cell = lstm
embed_size = 128
hidden_dim = 256
num_layers = 2
dropout_rate = 0.0

;model_type=RWKV
;n_layer = 6
;n_embd = 512

;model_type=gpt2
;model_name_or_path=gpt2
[dataset]
datafile = /data/lastness/LCCC-base/corpus.txt
datafile_encoding = utf-8
ctx_len = 128
is_uncase = true
word_level = false
[vocab]
vocab_size=30000
min_frequency=20
[trainer]
max_epochs = 500
epoch_length_fixed = 10000
epoch_save_frequency = 50
epoch_save_path = trained-
batch_size = 8
lr_init = 8e-4
lr_final = 1e-5
grad_norm_clip = 1.0
eps = 4e-9
num_workers = 0
[generate]
model_name=trained-1
[gpt2]
split=0.95
prompt=stega generation:
weight_decay=0.01
lr_scheduler_type=linear
warmup_ratio=0.06
GENERATE_EVERY=1000
EVAL_STEPS=1000

