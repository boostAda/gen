[model]
;model_type=RNN
;cell = lstm
;embed_size = 128
;hidden_dim = 256
;num_layers = 2
;dropout_rate = 0.0
;
model_type=RWKV
n_layer = 6
n_embd = 512

;model_type=gpt2
;model_name_or_path=gpt2
[dataset]
datafile = /data/lastness/LCCC-base/dialog.txt3000000
datafile_encoding = utf-8
ctx_len = 128
is_uncase = true
word_level = True
[vocab]
vocab_size=30000
min_frequency=1
[trainer]
max_epochs = 500
epoch_length_fixed = 10000
epoch_save_frequency = 50
epoch_save_path = trained-
batch_size = 32
lr_init = 8e-4
lr_final = 1e-5
grad_norm_clip = 1.0
eps = 4e-9
num_workers = 0
[generate]
bit_filepath=bit_stream.txt
model_name=trained-500
generate_num=100000
max_length=128
alg=hc
topp=0.9
bit=5
precision=52
epsilon=0.01
max_bit=15
[extract]
in_filepath=out/RWKV-trained-1-ac-100-128-precision52.txt
[gpt2]
split=0.95
prompt=stega generation:
weight_decay=0.01
lr_scheduler_type=linear
warmup_ratio=0.06
GENERATE_EVERY=1000
EVAL_STEPS=1000
[mix]
retrieval_filepath=/data/lastness/LCCC-base/corpus.txt1000000_do_shuffle


